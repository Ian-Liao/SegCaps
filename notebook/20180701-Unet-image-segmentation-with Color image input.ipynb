{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet on Image Segmentation for Person\n",
    "## Input Color image files map to 24 bits data space.\n",
    "## loss function = dice\n",
    "## 10 epochs, 1,000 iterations per epoch.\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment object of person.\n",
    "\n",
    "This notebook tests the model loading function from image file of a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from os.path import join, basename\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "import numpy as np\n",
    "# import skimage.io\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.utils import print_summary\n",
    "from keras import layers, models\n",
    "\n",
    "import segcapsnet.capsnet as modellib\n",
    "import models.unet as unet\n",
    "\n",
    "from utils.model_helper import create_model\n",
    "from utils.load_2D_data import generate_test_batches, generate_test_image\n",
    "from utils.custom_data_aug import augmentImages, process_image, image_resize2square\n",
    "from test import *\n",
    "from PIL import Image\n",
    "import scipy.ndimage.morphology\n",
    "from skimage import measure, filters\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "RESOLUTION = 512\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = path.dirname(\"../\")\n",
    "DATA_DIR = path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = path.join(DATA_DIR, \"saved_models/segcapsr3/m1.hdf5\") # LUNA16\n",
    "\n",
    "# Local path to trained weights file\n",
    "# loss function = Dice is better than BCE (Binary Cross Entropy)\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-055808.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/capsbasic/cb1.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/mar10-255.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/bce.hdf5\") # MSCOCO17\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/unet/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-083149.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = path.join(DATA_DIR, \"imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 512, 512, 64) 640         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 256, 256, 64) 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 128 73856       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 128 147584      conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, 128, 256 295168      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 128, 128, 256 590080      conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 64, 512)  1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 512)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 1024) 4719616     max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 1024) 9438208     conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 512)  2097664     conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 1024) 0           conv2d_transpose_5[0][0]         \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 256 524544      conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 512 0           conv2d_transpose_6[0][0]         \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, 128, 256 590080      conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 128 131200      conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 256 0           conv2d_transpose_7[0][0]         \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 128 295040      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, 256, 128 147584      conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 512, 512, 64) 32832       conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 512, 512, 128 0           conv2d_transpose_8[0][0]         \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 512, 512, 1)  65          conv2d_37[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,030,593\n",
      "Trainable params: 31,030,593\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "num_class = 2\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetBasic(net_input_shape, num_class)\n",
    "eval_model = unet.UNet(net_input_shape)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "eval_model.load_weights(COCO_MODEL_PATH)\n",
    "print_summary(model=eval_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_mask(raw_output, threshold):\n",
    "    if threshold == 0:\n",
    "        try:\n",
    "            threshold = filters.threshold_otsu(raw_output)\n",
    "        except:\n",
    "            threshold = 0.5\n",
    "\n",
    "    print('\\tThreshold: {}'.format(threshold))\n",
    "\n",
    "    raw_output[raw_output > threshold] = 1\n",
    "    raw_output[raw_output < 1] = 0\n",
    "\n",
    "    all_labels = measure.label(raw_output)\n",
    "    props = measure.regionprops(all_labels)\n",
    "    props.sort(key=lambda x: x.area, reverse=True)\n",
    "    thresholded_mask = np.zeros(raw_output.shape)\n",
    "\n",
    "    if len(props) >= 2:\n",
    "        if props[0].area / props[1].area > 5:  # if the largest is way larger than the second largest\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # only turn on the largest component\n",
    "        else:\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # turn on two largest components\n",
    "            thresholded_mask[all_labels == props[1].label] = 1\n",
    "    elif len(props):\n",
    "        thresholded_mask[all_labels == props[0].label] = 1\n",
    "\n",
    "    thresholded_mask = scipy.ndimage.morphology.binary_fill_holes(thresholded_mask).astype(np.uint8)\n",
    "\n",
    "    return thresholded_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Segmentation of Person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-02 10:00:12.619198\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "2018-07-02 10:00:17.565346\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = ['train2.png']\n",
    "output_array = None\n",
    "\n",
    "\n",
    "# sitk_img = sitk.ReadImage(join(IMAGE_DIR, img[0]))\n",
    "# img_data = sitk.GetArrayFromImage(sitk_img)\n",
    "img_data = np.array(Image.open(join(IMAGE_DIR, img[0])))\n",
    "    \n",
    "print(str(datetime.now()))\n",
    "output_array = eval_model.predict_generator(generate_test_image(img_data,\n",
    "                                                                  net_input_shape,\n",
    "                                                                  batchSize=1,\n",
    "                                                                  numSlices=1,\n",
    "                                                                  subSampAmt=0,\n",
    "                                                                  stride=1),\n",
    "                                            steps=1, max_queue_size=1, workers=1,\n",
    "                                            use_multiprocessing=False, verbose=1)\n",
    "# output_array = eval_model.predict_generator(generate_test_batches(DATA_DIR, [img],\n",
    "#                                                                   net_input_shape,\n",
    "#                                                                   batchSize=1,\n",
    "#                                                                   numSlices=1,\n",
    "#                                                                   subSampAmt=0,\n",
    "#                                                                   stride=1),\n",
    "#                                             steps=1, max_queue_size=1, workers=1,\n",
    "#                                             use_multiprocessing=False, verbose=1)\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_array contain 2 masks in a list, show the first element.\n",
    "# print('len(output_array)=%d'%(len(output_array)))\n",
    "# print('test.test: output_array=%s'%(output_array[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.test: output=[[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# output = (1, 512, 512)\n",
    "# output = output_array[0][:,:,:,0] # A list with two images, get first one image and reshape it to 3 dimensions.\n",
    "# recon = output_array[1][:,:,:,0]\n",
    "\n",
    "# For unet\n",
    "output = output_array[:,:,:,0]\n",
    "# image store in tuple structure.\n",
    "print('test.test: output=%s'%(output))\n",
    "np.ndim(output)\n",
    "np_output = np.array(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting Output\n",
      "\tThreshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "# output_img = sitk.GetImageFromArray(output[0,:,:], isVector=True)\n",
    "\n",
    "print('Segmenting Output')\n",
    "threshold_level = 0\n",
    "output_bin = threshold_mask(output, threshold_level)\n",
    "# output2d = output[0,:,:]\n",
    "# output2d = recon[0,:,:]\n",
    "# print(output2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b87d3f748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(output[0,:,:], cmap='gray')\n",
    "# plt.imsave('raw_output' + img[0][-4:], output[0,:,:])\n",
    "plt.imshow(output_bin[0,:,:], cmap='gray')\n",
    "plt.imsave('unet-lr-0.001-iter1000-ep1' + img[0][-4:], output_bin[0,:,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
