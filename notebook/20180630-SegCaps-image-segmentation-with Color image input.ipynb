{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegCaps on Image Segmentation for Person\n",
    "## Input color map to 24 bits grayscaleimage files\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment object of person.\n",
    "\n",
    "This notebook tests the model loading function from image file of a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "from os.path import join, basename\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "import numpy as np\n",
    "# import skimage.io\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.utils import print_summary\n",
    "from keras import layers, models\n",
    "\n",
    "import segcapsnet.capsnet as modellib\n",
    "import models.unet as unet\n",
    "\n",
    "from utils.model_helper import create_model\n",
    "from utils.load_2D_data import generate_test_batches, generate_test_image\n",
    "from utils.custom_data_aug import augmentImages, process_image, image_resize2square\n",
    "from test import *\n",
    "from PIL import Image\n",
    "import scipy.ndimage.morphology\n",
    "from skimage import measure, filters\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "RESOLUTION = 512\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = path.dirname(\"../\")\n",
    "DATA_DIR = path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = path.join(DATA_DIR, \"saved_models/segcapsr3/m1.hdf5\") # LUNA16\n",
    "\n",
    "# Local path to trained weights file\n",
    "# loss function = Dice is better than BCE (Binary Cross Entropy)\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-055808.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/capsbasic/cb1.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/mar10-255.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/bce.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/unet/unet1.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = path.join(DATA_DIR, \"imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 512, 16) 416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 512, 512, 1,  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycaps (ConvCapsuleLayer)  (None, 256, 256, 2,  12832       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_1 (ConvCapsuleLayer) (None, 256, 256, 4,  25664       primarycaps[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_2 (ConvCapsuleLayer) (None, 128, 128, 4,  51328       conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_1 (ConvCapsuleLayer) (None, 128, 128, 8,  205056      conv_cap_2_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_2 (ConvCapsuleLayer) (None, 64, 64, 8, 64 410112      conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_4_1 (ConvCapsuleLayer) (None, 64, 64, 8, 32 409856      conv_cap_3_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_1 (DeconvCapsuleLa (None, 128, 128, 8,  131328      conv_cap_4_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_1 (Concatenate)              (None, 128, 128, 16, 0           deconv_cap_1_1[0][0]             \n",
      "                                                                 conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_2 (ConvCapsuleLaye (None, 128, 128, 4,  102528      up_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_1 (DeconvCapsuleLa (None, 256, 256, 4,  32832       deconv_cap_1_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_2 (Concatenate)              (None, 256, 256, 8,  0           deconv_cap_2_1[0][0]             \n",
      "                                                                 conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_2 (ConvCapsuleLaye (None, 256, 256, 4,  25664       up_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_3_1 (DeconvCapsuleLa (None, 512, 512, 2,  8224        deconv_cap_2_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_3 (Concatenate)              (None, 512, 512, 3,  0           deconv_cap_3_1[0][0]             \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seg_caps (ConvCapsuleLayer)     (None, 512, 512, 1,  272         up_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "mask_2 (Mask)                   (None, 512, 512, 1,  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 512, 512, 16) 0           mask_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "recon_1 (Conv2D)                (None, 512, 512, 64) 1088        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "recon_2 (Conv2D)                (None, 512, 512, 128 8320        recon_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_seg (Length)                (None, 512, 512, 1)  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Conv2D)              (None, 512, 512, 1)  129         recon_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,425,649\n",
      "Trainable params: 1,425,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "num_class = 2\n",
    "train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetBasic(net_input_shape, num_class)\n",
    "# eval_model = unet.UNet(net_input_shape)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "eval_model.load_weights(COCO_MODEL_PATH)\n",
    "print_summary(model=eval_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_mask(raw_output, threshold):\n",
    "    if threshold == 0:\n",
    "        try:\n",
    "            threshold = filters.threshold_otsu(raw_output)\n",
    "        except:\n",
    "            threshold = 0.5\n",
    "\n",
    "    print('\\tThreshold: {}'.format(threshold))\n",
    "\n",
    "    raw_output[raw_output > threshold] = 1\n",
    "    raw_output[raw_output < 1] = 0\n",
    "\n",
    "    all_labels = measure.label(raw_output)\n",
    "    props = measure.regionprops(all_labels)\n",
    "    props.sort(key=lambda x: x.area, reverse=True)\n",
    "    thresholded_mask = np.zeros(raw_output.shape)\n",
    "\n",
    "    if len(props) >= 2:\n",
    "        if props[0].area / props[1].area > 5:  # if the largest is way larger than the second largest\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # only turn on the largest component\n",
    "        else:\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # turn on two largest components\n",
    "            thresholded_mask[all_labels == props[1].label] = 1\n",
    "    elif len(props):\n",
    "        thresholded_mask[all_labels == props[0].label] = 1\n",
    "\n",
    "    thresholded_mask = scipy.ndimage.morphology.binary_fill_holes(thresholded_mask).astype(np.uint8)\n",
    "\n",
    "    return thresholded_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Segmentation of Person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-01 23:32:54.563964\n",
      "1/1 [==============================] - 42s 42s/step\n",
      "2018-07-01 23:33:36.547337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = ['train2.png']\n",
    "output_array = None\n",
    "\n",
    "\n",
    "# sitk_img = sitk.ReadImage(join(IMAGE_DIR, img[0]))\n",
    "# img_data = sitk.GetArrayFromImage(sitk_img)\n",
    "img_data = np.array(Image.open(join(IMAGE_DIR, img[0])))\n",
    "    \n",
    "print(str(datetime.now()))\n",
    "output_array = eval_model.predict_generator(generate_test_batches(DATA_DIR, [img],\n",
    "                                                                  net_input_shape,\n",
    "                                                                  batchSize=1,\n",
    "                                                                  numSlices=1,\n",
    "                                                                  subSampAmt=0,\n",
    "                                                                  stride=1),\n",
    "                                            steps=1, max_queue_size=1, workers=1,\n",
    "                                            use_multiprocessing=False, verbose=1)\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_array contain 2 masks in a list, show the first element.\n",
    "# print('len(output_array)=%d'%(len(output_array)))\n",
    "# print('test.test: output_array=%s'%(output_array[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.test: output=[[[0.4349749  0.4559768  0.448838   ... 0.4442466  0.46016315 0.45820037]\n",
      "  [0.4521037  0.543738   0.51176393 ... 0.46821088 0.485114   0.42456394]\n",
      "  [0.4761315  0.55155635 0.54252845 ... 0.48483464 0.5148635  0.47889495]\n",
      "  ...\n",
      "  [0.4797593  0.42427978 0.4150163  ... 0.47550964 0.47368604 0.50004005]\n",
      "  [0.5315075  0.46919912 0.5075262  ... 0.50883406 0.49736318 0.51616526]\n",
      "  [0.54227406 0.51487434 0.54154915 ... 0.5500171  0.53233016 0.5048668 ]]]\n"
     ]
    }
   ],
   "source": [
    "# output = (1, 512, 512)\n",
    "output = output_array[0][:,:,:,0] # A list with two images, get first one image and reshape it to 3 dimensions.\n",
    "recon = output_array[1][:,:,:,0]\n",
    "\n",
    "# For unet\n",
    "# output = output_array[:,:,:,0]\n",
    "# image store in tuple structure.\n",
    "print('test.test: output=%s'%(output))\n",
    "np.ndim(output)\n",
    "np_output = np.array(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting Output\n",
      "\tThreshold: 0.6059660543105565\n"
     ]
    }
   ],
   "source": [
    "# output_img = sitk.GetImageFromArray(output[0,:,:], isVector=True)\n",
    "\n",
    "print('Segmenting Output')\n",
    "threshold_level = 0\n",
    "output_bin = threshold_mask(output, threshold_level)\n",
    "# output2d = output[0,:,:]\n",
    "# output2d = recon[0,:,:]\n",
    "# print(output2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fcb656e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(output[0,:,:], cmap='gray')\n",
    "# plt.imsave('raw_output' + img[0][-4:], output[0,:,:])\n",
    "plt.imshow(output_bin[0,:,:], cmap='gray')\n",
    "plt.imsave('segcapsr3-lr-0.001-iter1001-ep4' + img[0][-4:], output_bin[0,:,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
