{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegCapsR3 on Image Segmentation for Person\n",
    "## Input Color image files map to 24 bits data space.\n",
    "## loss function = dice\n",
    "## 10 epochs, 1,000 iterations per epoch.\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment object of person.\n",
    "\n",
    "This notebook tests the model loading function from image file of a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "from os.path import join, basename\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "import numpy as np\n",
    "# import skimage.io\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.utils import print_summary\n",
    "from keras import layers, models\n",
    "\n",
    "import segcapsnet.capsnet as modellib\n",
    "import models.unet as unet\n",
    "\n",
    "from utils.model_helper import create_model\n",
    "from utils.load_2D_data import generate_test_batches, generate_test_image\n",
    "from utils.custom_data_aug import augmentImages, process_image, image_resize2square\n",
    "from test import *\n",
    "from PIL import Image\n",
    "import scipy.ndimage.morphology\n",
    "from skimage import measure, filters\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "RESOLUTION = 512\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = path.dirname(\"../\")\n",
    "DATA_DIR = path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = path.join(DATA_DIR, \"saved_models/segcapsr3/m1.hdf5\") # LUNA16\n",
    "\n",
    "# Local path to trained weights file\n",
    "# loss function = Dice is better than BCE (Binary Cross Entropy)\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-055808.hdf5\") # MSCOCO17\n",
    "\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.001_recon-5.0_model_20180703-152449.hdf5\") # poor\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180703-180852.hdf5\") # good\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180703-210306.hdf5\") # ?\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-20.0_model_20180703-221150.hdf5\") # ?\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.1_recon-20.0_model_20180703-225112.hdf5\") # better\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.0001_recon-20.0_model_20180703-234853.hdf5\") # best\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.001_recon-0.5_model_20180704-030457.hdf5\") # ok\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-0_loss-dice_slic-1_sub--1_strid-1_lr-0.0001_recon-20.0_model_20180705-092846.hdf5\") # best\n",
    "\n",
    "\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/capsbasic/cb1.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/mar10-255.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/bce.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/unet/unet1.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = path.join(DATA_DIR, \"imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 512, 16) 416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 512, 512, 1,  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycaps (ConvCapsuleLayer)  (None, 256, 256, 2,  12832       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_1 (ConvCapsuleLayer) (None, 256, 256, 4,  25664       primarycaps[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_2_2 (ConvCapsuleLayer) (None, 128, 128, 4,  51328       conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_1 (ConvCapsuleLayer) (None, 128, 128, 8,  205056      conv_cap_2_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_3_2 (ConvCapsuleLayer) (None, 64, 64, 8, 64 410112      conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_cap_4_1 (ConvCapsuleLayer) (None, 64, 64, 8, 32 409856      conv_cap_3_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_1 (DeconvCapsuleLa (None, 128, 128, 8,  131328      conv_cap_4_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_1 (Concatenate)              (None, 128, 128, 16, 0           deconv_cap_1_1[0][0]             \n",
      "                                                                 conv_cap_3_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_1_2 (ConvCapsuleLaye (None, 128, 128, 4,  102528      up_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_1 (DeconvCapsuleLa (None, 256, 256, 4,  32832       deconv_cap_1_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_2 (Concatenate)              (None, 256, 256, 8,  0           deconv_cap_2_1[0][0]             \n",
      "                                                                 conv_cap_2_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_2_2 (ConvCapsuleLaye (None, 256, 256, 4,  25664       up_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deconv_cap_3_1 (DeconvCapsuleLa (None, 512, 512, 2,  8224        deconv_cap_2_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_3 (Concatenate)              (None, 512, 512, 3,  0           deconv_cap_3_1[0][0]             \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seg_caps (ConvCapsuleLayer)     (None, 512, 512, 1,  272         up_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "mask_2 (Mask)                   (None, 512, 512, 1,  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 512, 512, 16) 0           mask_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "recon_1 (Conv2D)                (None, 512, 512, 64) 1088        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "recon_2 (Conv2D)                (None, 512, 512, 128 8320        recon_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_seg (Length)                (None, 512, 512, 1)  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Conv2D)              (None, 512, 512, 1)  129         recon_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,425,649\n",
      "Trainable params: 1,425,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "num_class = 2\n",
    "train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetBasic(net_input_shape, num_class)\n",
    "# eval_model = unet.UNet(net_input_shape)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "eval_model.load_weights(COCO_MODEL_PATH)\n",
    "print_summary(model=eval_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_mask(raw_output, threshold):\n",
    "    if threshold == 0:\n",
    "        try:\n",
    "            threshold = filters.threshold_otsu(raw_output)\n",
    "        except:\n",
    "            threshold = 0.5\n",
    "\n",
    "    print('\\tThreshold: {}'.format(threshold))\n",
    "\n",
    "    raw_output[raw_output > threshold] = 1\n",
    "    raw_output[raw_output < 1] = 0\n",
    "\n",
    "    all_labels = measure.label(raw_output)\n",
    "    props = measure.regionprops(all_labels)\n",
    "    props.sort(key=lambda x: x.area, reverse=True)\n",
    "    thresholded_mask = np.zeros(raw_output.shape)\n",
    "\n",
    "    if len(props) >= 2:\n",
    "        if props[0].area / props[1].area > 5:  # if the largest is way larger than the second largest\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # only turn on the largest component\n",
    "        else:\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # turn on two largest components\n",
    "            thresholded_mask[all_labels == props[1].label] = 1\n",
    "    elif len(props):\n",
    "        thresholded_mask[all_labels == props[0].label] = 1\n",
    "\n",
    "    thresholded_mask = scipy.ndimage.morphology.binary_fill_holes(thresholded_mask).astype(np.uint8)\n",
    "\n",
    "    return thresholded_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Segmentation of Person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-07 09:39:54.357989\n",
      "1/1 [==============================] - 46s 46s/step\n",
      "2018-07-07 09:40:40.381549\n"
     ]
    }
   ],
   "source": [
    "img = ['train1.png']\n",
    "# img = ['train-i-0.png']\n",
    "# img = ['experiment/train2.png']\n",
    "output_array = None\n",
    "\n",
    "\n",
    "# sitk_img = sitk.ReadImage(join(IMAGE_DIR, img[0]))\n",
    "# img_data = sitk.GetArrayFromImage(sitk_img)\n",
    "img_data = np.array(Image.open(join(IMAGE_DIR, img[0])))\n",
    "    \n",
    "print(str(datetime.now()))\n",
    "output_array = eval_model.predict_generator(generate_test_image(img_data,\n",
    "                                                                  net_input_shape,\n",
    "                                                                  batchSize=1,\n",
    "                                                                  numSlices=1,\n",
    "                                                                  subSampAmt=0,\n",
    "                                                                  stride=1),\n",
    "                                            steps=1, max_queue_size=1, workers=1,\n",
    "                                            use_multiprocessing=False, verbose=1)\n",
    "\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_array contain 2 masks in a list, show the first element.\n",
    "# print('len(output_array)=%d'%(len(output_array)))\n",
    "# print('test.test: output_array=%s'%(output_array[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.test: output=[[[0.45679113 0.47127154 0.47420034 ... 0.47056085 0.476707   0.44596344]\n",
      "  [0.45354244 0.4923004  0.50066    ... 0.52313673 0.5206716  0.4923097 ]\n",
      "  [0.46662143 0.49274352 0.50101686 ... 0.49266818 0.51320624 0.4601742 ]\n",
      "  ...\n",
      "  [0.47809583 0.5183113  0.5074172  ... 0.49978504 0.5070888  0.5099464 ]\n",
      "  [0.492699   0.50687975 0.5185486  ... 0.5102853  0.5101571  0.49890918]\n",
      "  [0.5134815  0.5011084  0.5136996  ... 0.52847296 0.503086   0.4882646 ]]]\n"
     ]
    }
   ],
   "source": [
    "# output = (1, 512, 512)\n",
    "output = output_array[0][:,:,:,0] # A list with two images, get first one image and reshape it to 3 dimensions.\n",
    "recon = output_array[1][:,:,:,0]\n",
    "\n",
    "# For unet\n",
    "# output = output_array[:,:,:,0]\n",
    "# image store in tuple structure.\n",
    "print('test.test: output=%s'%(output))\n",
    "np.ndim(output)\n",
    "np_output = np.array(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting Output\n",
      "\tThreshold: 0.6128642031690106\n"
     ]
    }
   ],
   "source": [
    "# output_img = sitk.GetImageFromArray(output[0,:,:], isVector=True)\n",
    "\n",
    "print('Segmenting Output')\n",
    "threshold_level = 0\n",
    "output_bin = threshold_mask(output, threshold_level)\n",
    "# output2d = output[0,:,:]\n",
    "# output2d = recon[0,:,:]\n",
    "# print(output2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22f0bf18518>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22f0c117710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imsave('raw_output' + img[0][-4:], output[0,:,:])\n",
    "plt.imshow(output_bin[0,:,:], cmap='gray')\n",
    "# plt.imsave('segcapsr3-lr-0.001-iter1000-ep7' + img[0][-4:], output_bin[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
