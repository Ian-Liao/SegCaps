{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegCaps on Image Segmentation for Person\n",
    "## Input Color image files\n",
    "## Integrated with WebCam Video\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment object of person.\n",
    "\n",
    "This notebook tests the model loading function from image file of a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "from os.path import join, basename\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "import numpy as np\n",
    "# import skimage.io\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.utils import print_summary\n",
    "from keras import layers, models\n",
    "\n",
    "import segcapsnet.capsnet as modellib\n",
    "import models.unet as unet\n",
    "\n",
    "from utils.model_helper import create_model\n",
    "from utils.load_2D_data import generate_test_batches, generate_test_image\n",
    "from test import *\n",
    "from PIL import Image\n",
    "import scipy.ndimage.morphology\n",
    "from skimage import measure, filters\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "RESOLUTION = 512\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = path.dirname(\"../\")\n",
    "DATA_DIR = path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = path.join(DATA_DIR, \"saved_models/segcapsr3/m1.hdf5\") # LUNA16\n",
    "\n",
    "# Local path to trained weights file\n",
    "# loss function = Dice is better than BCE (Binary Cross Entropy)\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/dice16-255.hdf5\") # MSCOCO17\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/capsbasic/cb1.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/mar10-255.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/bce.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/unet/unet1.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = path.join(DATA_DIR, \"imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../segcapsnet/capsule_layers.py:322: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../segcapsnet/capsule_layers.py:322: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../segcapsnet/capsule_layers.py:351: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../segcapsnet/capsule_layers.py:351: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 512, 256 6656        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 512, 512, 1,  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycaps (ConvCapsuleLayer)  (None, 512, 512, 8,  1638656     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seg_caps (ConvCapsuleLayer)     (None, 512, 512, 1,  528         primarycaps[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mask_2 (Mask)                   (None, 512, 512, 1,  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 512, 512, 16) 0           mask_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "recon_1 (Conv2D)                (None, 512, 512, 64) 1088        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "recon_2 (Conv2D)                (None, 512, 512, 128 8320        recon_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_seg (Length)                (None, 512, 512, 1)  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Conv2D)              (None, 512, 512, 1)  129         recon_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,655,377\n",
      "Trainable params: 1,655,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "num_class = 2\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "train_model, eval_model, manipulate_model = modellib.CapsNetBasic(net_input_shape, num_class)\n",
    "# eval_model = unet.UNet(net_input_shape)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "eval_model.load_weights(COCO_MODEL_PATH)\n",
    "print_summary(model=eval_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_mask(raw_output, threshold):\n",
    "    if threshold == 0:\n",
    "        try:\n",
    "            threshold = filters.threshold_otsu(raw_output)\n",
    "        except:\n",
    "            threshold = 0.5\n",
    "\n",
    "    print('\\tThreshold: {}'.format(threshold))\n",
    "\n",
    "    raw_output[raw_output > threshold] = 1\n",
    "    raw_output[raw_output < 1] = 0\n",
    "\n",
    "    all_labels = measure.label(raw_output)\n",
    "    props = measure.regionprops(all_labels)\n",
    "    props.sort(key=lambda x: x.area, reverse=True)\n",
    "    thresholded_mask = np.zeros(raw_output.shape)\n",
    "\n",
    "    if len(props) >= 2:\n",
    "        if props[0].area / props[1].area > 5:  # if the largest is way larger than the second largest\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # only turn on the largest component\n",
    "        else:\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # turn on two largest components\n",
    "            thresholded_mask[all_labels == props[1].label] = 1\n",
    "    elif len(props):\n",
    "        thresholded_mask[all_labels == props[0].label] = 1\n",
    "\n",
    "    thresholded_mask = scipy.ndimage.morphology.binary_fill_holes(thresholded_mask).astype(np.uint8)\n",
    "\n",
    "    return thresholded_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Segmentation of Person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-30 07:45:13.122611\n",
      "1/1 [==============================] - 40s 40s/step\n",
      "2018-06-30 07:45:52.984257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = ['train2.png']\n",
    "output_array = None\n",
    "\n",
    "\n",
    "# sitk_img = sitk.ReadImage(join(IMAGE_DIR, img[0]))\n",
    "# img_data = sitk.GetArrayFromImage(sitk_img)\n",
    "img_data = np.asarray(Image.open(join(IMAGE_DIR, img[0])))\n",
    "\n",
    "    \n",
    "print(str(datetime.now()))\n",
    "output_array = eval_model.predict_generator(generate_test_batches(DATA_DIR, [img],\n",
    "                                                                  net_input_shape,\n",
    "                                                                  batchSize=1,\n",
    "                                                                  numSlices=1,\n",
    "                                                                  subSampAmt=0,\n",
    "                                                                  stride=1),\n",
    "                                            steps=1, max_queue_size=1, workers=1,\n",
    "                                            use_multiprocessing=False, verbose=1)\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_array contain 2 masks in a list, show the first element.\n",
    "# print('len(output_array)=%d'%(len(output_array)))\n",
    "# print('test.test: output_array=%s'%(output_array[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.test: output=[[[0.999551   0.9995523  0.99955237 ... 0.9995541  0.9995544  0.99955344]\n",
      "  [0.99955153 0.9995528  0.9995528  ... 0.9995547  0.999555   0.9995539 ]\n",
      "  [0.9995511  0.99955255 0.99955255 ... 0.99955446 0.9995547  0.9995538 ]\n",
      "  ...\n",
      "  [0.9995482  0.9995497  0.9995498  ... 0.9995611  0.99956137 0.9995608 ]\n",
      "  [0.9995478  0.9995491  0.9995493  ... 0.9995608  0.9995611  0.9995604 ]\n",
      "  [0.9995464  0.99954784 0.9995479  ... 0.9995595  0.99956    0.99955934]]]\n"
     ]
    }
   ],
   "source": [
    "# output = (1, 512, 512)\n",
    "output = output_array[0][:,:,:,0] # A list with two images, get first one image and reshape it to 3 dimensions.\n",
    "recon = output_array[1][:,:,:,0]\n",
    "\n",
    "# For unet\n",
    "# output = output_array[:,:,:,0]\n",
    "# image store in tuple structure.\n",
    "print('test.test: output=%s'%(output))\n",
    "np.ndim(output)\n",
    "np_output = np.array(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting Output\n",
      "\tThreshold: 0.9995464934036136\n"
     ]
    }
   ],
   "source": [
    "# output_img = sitk.GetImageFromArray(output[0,:,:], isVector=True)\n",
    "\n",
    "print('Segmenting Output')\n",
    "threshold_level = 0\n",
    "output_bin = threshold_mask(output, threshold_level)\n",
    "# output2d = output[0,:,:]\n",
    "# output2d = recon[0,:,:]\n",
    "# print(output2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0345b9c748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0345bb9630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(output[0,:,:], cmap='gray')\n",
    "# plt.imsave('raw_output' + img[0][-4:], output[0,:,:])\n",
    "plt.imshow(output_bin[0,:,:], cmap='gray')\n",
    "# plt.imsave('final_output' + img[0][-4:], output_bin[0,:,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
