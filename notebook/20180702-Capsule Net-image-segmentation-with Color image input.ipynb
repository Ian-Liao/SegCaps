{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Nets on Image Segmentation for Person\n",
    "## Input Color image files map to 24 bits Gray Scale data space.\n",
    "## loss function = dice\n",
    "## 10 epochs, 1,000 iterations per epoch.\n",
    "\n",
    "A quick intro to using the pre-trained model to detect and segment object of person.\n",
    "\n",
    "This notebook tests the model loading function from image file of a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from os.path import join, basename\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "import numpy as np\n",
    "# import skimage.io\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('../')\n",
    "\n",
    "from keras.utils import print_summary\n",
    "from keras import layers, models\n",
    "\n",
    "import segcapsnet.capsnet as modellib\n",
    "import models.unet as unet\n",
    "\n",
    "from utils.model_helper import create_model\n",
    "from utils.load_2D_data import generate_test_batches, generate_test_image\n",
    "from utils.custom_data_aug import augmentImages, process_image, image_resize2square\n",
    "from test import *\n",
    "from PIL import Image\n",
    "import scipy.ndimage.morphology\n",
    "from skimage import measure, filters\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "RESOLUTION = 512\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = path.dirname(\"../\")\n",
    "DATA_DIR = path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "# MODEL_DIR = path.join(DATA_DIR, \"saved_models/segcapsr3/m1.hdf5\") # LUNA16\n",
    "\n",
    "# Local path to trained weights file\n",
    "# loss function = Dice is better than BCE (Binary Cross Entropy)\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-055808.hdf5\") # MSCOCO17\n",
    "COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/capsbasic/split-0_batch-1_shuff-1_aug-1_loss-dice_slic-1_sub--1_strid-1_lr-0.01_recon-2.0_model_20180702-135850.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/mar10-255.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/segcapsr3/bce.hdf5\") # MSCOCO17\n",
    "# COCO_MODEL_PATH = path.join(DATA_DIR, \"saved_models/unet/unet1.hdf5\") # MSCOCO17\n",
    "\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = path.join(DATA_DIR, \"imgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Load Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 512, 512, 256 6656        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 512, 512, 1,  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycaps (ConvCapsuleLayer)  (None, 512, 512, 8,  1638656     reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "seg_caps (ConvCapsuleLayer)     (None, 512, 512, 1,  528         primarycaps[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mask_11 (Mask)                  (None, 512, 512, 1,  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 512, 512, 16) 0           mask_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "recon_1 (Conv2D)                (None, 512, 512, 64) 1088        reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "recon_2 (Conv2D)                (None, 512, 512, 128 8320        recon_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_seg (Length)                (None, 512, 512, 1)  0           seg_caps[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Conv2D)              (None, 512, 512, 1)  129         recon_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,655,377\n",
      "Trainable params: 1,655,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "net_input_shape = (RESOLUTION, RESOLUTION, 1)\n",
    "num_class = 2\n",
    "# train_model, eval_model, manipulate_model = modellib.CapsNetR3(net_input_shape, num_class)\n",
    "train_model, eval_model, manipulate_model = modellib.CapsNetBasic(net_input_shape, num_class)\n",
    "# eval_model = unet.UNet(net_input_shape)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "eval_model.load_weights(COCO_MODEL_PATH)\n",
    "print_summary(model=eval_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_mask(raw_output, threshold):\n",
    "    if threshold == 0:\n",
    "        try:\n",
    "            threshold = filters.threshold_otsu(raw_output)\n",
    "        except:\n",
    "            threshold = 0.5\n",
    "\n",
    "    print('\\tThreshold: {}'.format(threshold))\n",
    "\n",
    "    raw_output[raw_output > threshold] = 1\n",
    "    raw_output[raw_output < 1] = 0\n",
    "\n",
    "    all_labels = measure.label(raw_output)\n",
    "    props = measure.regionprops(all_labels)\n",
    "    props.sort(key=lambda x: x.area, reverse=True)\n",
    "    thresholded_mask = np.zeros(raw_output.shape)\n",
    "\n",
    "    if len(props) >= 2:\n",
    "        if props[0].area / props[1].area > 5:  # if the largest is way larger than the second largest\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # only turn on the largest component\n",
    "        else:\n",
    "            thresholded_mask[all_labels == props[0].label] = 1  # turn on two largest components\n",
    "            thresholded_mask[all_labels == props[1].label] = 1\n",
    "    elif len(props):\n",
    "        thresholded_mask[all_labels == props[0].label] = 1\n",
    "\n",
    "    thresholded_mask = scipy.ndimage.morphology.binary_fill_holes(thresholded_mask).astype(np.uint8)\n",
    "\n",
    "    return thresholded_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Segmentation of Person\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-02 09:49:28.387645\n",
      "1/1 [==============================] - 20s 20s/step\n",
      "2018-07-02 09:49:48.541667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img = ['train2.png']\n",
    "output_array = None\n",
    "\n",
    "\n",
    "# sitk_img = sitk.ReadImage(join(IMAGE_DIR, img[0]))\n",
    "# img_data = sitk.GetArrayFromImage(sitk_img)\n",
    "img_data = np.array(Image.open(join(IMAGE_DIR, img[0])))\n",
    "    \n",
    "print(str(datetime.now()))\n",
    "output_array = eval_model.predict_generator(generate_test_image(img_data,\n",
    "                                                                  net_input_shape,\n",
    "                                                                  batchSize=1,\n",
    "                                                                  numSlices=1,\n",
    "                                                                  subSampAmt=0,\n",
    "                                                                  stride=1),\n",
    "                                            steps=1, max_queue_size=1, workers=1,\n",
    "                                            use_multiprocessing=False, verbose=1)\n",
    "# output_array = eval_model.predict_generator(generate_test_batches(DATA_DIR, [img],\n",
    "#                                                                   net_input_shape,\n",
    "#                                                                   batchSize=1,\n",
    "#                                                                   numSlices=1,\n",
    "#                                                                   subSampAmt=0,\n",
    "#                                                                   stride=1),\n",
    "#                                             steps=1, max_queue_size=1, workers=1,\n",
    "#                                             use_multiprocessing=False, verbose=1)\n",
    "print(str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_array contain 2 masks in a list, show the first element.\n",
    "# print('len(output_array)=%d'%(len(output_array)))\n",
    "# print('test.test: output_array=%s'%(output_array[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.test: output=[[[0.6620876  0.6082323  0.61582536 ... 0.53869915 0.5720225  0.6547375 ]\n",
      "  [0.54941297 0.52451104 0.58748585 ... 0.53559804 0.5170619  0.5595522 ]\n",
      "  [0.44549635 0.49041876 0.6090267  ... 0.58132124 0.5239647  0.5076793 ]\n",
      "  ...\n",
      "  [0.48975685 0.48427686 0.5923507  ... 0.41190243 0.44796765 0.47991556]\n",
      "  [0.5814096  0.51929194 0.559212   ... 0.41888505 0.5027385  0.5668226 ]\n",
      "  [0.66378087 0.5802719  0.55839187 ... 0.47253245 0.5759919  0.65410864]]]\n"
     ]
    }
   ],
   "source": [
    "# output = (1, 512, 512)\n",
    "output = output_array[0][:,:,:,0] # A list with two images, get first one image and reshape it to 3 dimensions.\n",
    "recon = output_array[1][:,:,:,0]\n",
    "\n",
    "# For unet\n",
    "# output = output_array[:,:,:,0]\n",
    "# image store in tuple structure.\n",
    "print('test.test: output=%s'%(output))\n",
    "np.ndim(output)\n",
    "np_output = np.array(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting Output\n",
      "\tThreshold: 0.6267912128241733\n"
     ]
    }
   ],
   "source": [
    "# output_img = sitk.GetImageFromArray(output[0,:,:], isVector=True)\n",
    "\n",
    "print('Segmenting Output')\n",
    "threshold_level = 0\n",
    "output_bin = threshold_mask(output, threshold_level)\n",
    "# output2d = output[0,:,:]\n",
    "# output2d = recon[0,:,:]\n",
    "# print(output2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25e81106240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(output[0,:,:], cmap='gray')\n",
    "# plt.imsave('raw_output' + img[0][-4:], output[0,:,:])\n",
    "plt.imshow(output_bin[0,:,:], cmap='gray')\n",
    "plt.imsave('CapsNetBasic-lr-0.001-iter1000-ep7' + img[0][-4:], output_bin[0,:,:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
